\documentclass[a4paper,11pt]{article}
\usepackage[cm]{fullpage}
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\of}[1]{\left( {#1} \right)}
\newcommand{\ofs}[1]{\left[ {#1} \right]}
\newcommand{\fracd}[2]{\frac{\dd{}{#1}}{\dd{}{#2}}}
\newcommand{\fracp}[2]{\frac{\partial{}{#1}}{\partial{}{#2}}}

\newcommand{\dd}{\text{d}}
\newcommand{\ee}{\text{e}}

\begin{document}
\section*{Gradient derivation for a two-layer network}
Consider the network with two inputs $x\in\mathbb{R}^2$ with {\bf two}
layers, the first one having {\bf three} neurons and the second, the
output layer, having just 1 neuron. The equations would be
\[
\begin{split}
z^1 &= W^1 x + b^1,\\
a^1 &= f_1\of{z^1},\\
z^2 &= W^2 x + b^2,\\
a^2 &= f_2(z^2),\\
\mathcal{L}\of{a^2, y} &= - \ofs{y\log a^2 + \of{1-y} \log\of{1 - a^2}}.
\end{split}
\]
Here, the superscript of $W$ and $b$ denotes the number of the layer.
The dimensions of the matrices are given by $W^1 \in
\mathbb{R}^{3\times 2}$, $b^1 \in \mathbb{R}^{3 \time 1}$, $W^2 \in
\mathbb{R}^{1 \times 3}$, and $b^2 \in \mathbb{R}^{1,1}$.

Component-wise, the above equations would read
\[
\begin{split}
z^1_1 &= W^1_{1,1} x_1 + W^1_{1,2} x_2 + b^1_{1,1},\\
z^1_2 &= W^1_{2,1} x_1 + W^1_{2,2} x_2 + b^1_{2,1},\\
z^1_3 &= W^1_{3,1} x_1 + W^1_{3,2} x_2 + b^1_{3,1},\\
a^1_1 &= f_1\of{z^1_1},\\
a^1_2 &= f_1\of{z^1_2},\\
a^1_3 &= f_1\of{z^1_3},\\
z^2_1 &= W^2_{1,2} a_1 + W^2_{1,2} a_2 + W^2_{1,3} a_3 + b^2_{1,1},\\
a^2_1 &= f_2\of{z^2_1},
\mathcal{L}\of{a^2,y} = -\ofs{y\log{a^2_1} + \of{1-y}\log\of{1 - a^2_1}}.
\end{split}
\]

\end{document}
